# The Research–Plan–Implement (RPI) Pattern in Complex Problem Solving and Collaboration

## Conceptual Foundations of the RPI Pattern

**RPI as a Cross-Disciplinary Pattern:** *Research–Plan–Implement* (RPI) is a cognitive and operational pattern that recurs across domains such as science, engineering, military strategy, and decision theory. At its core, RPI advocates distinct phases of **understanding a problem (Research), devising a solution approach (Plan), and executing that solution (Implement)**. This mirrors the structure of the scientific method – scientists first gather observations and background (research), then form hypotheses and experimental designs (plan), and finally conduct experiments and analyze results (implement and evaluate). Crucially, like the scientific method, RPI is *not* a rigid checklist of steps but a set of guiding principles[\[1\]](https://en.wikipedia.org/wiki/Scientific_method#:~:text=While%20the%20scientific%20method%20is,have%20not%20followed%20the%20textbook). In practice, not every problem requires each phase in strict order, and iterations or feedback loops are common. This flexibility is what makes RPI a **pattern** rather than a fixed linear process. It provides a template that can repeat recursively at different scales (for example, within a large project’s implementation phase, one might do a smaller RPI cycle to solve a sub-problem), much like how real scientific inquiry often revisits earlier steps[\[1\]](https://en.wikipedia.org/wiki/Scientific_method#:~:text=While%20the%20scientific%20method%20is,have%20not%20followed%20the%20textbook).

**Historical Roots:** Variants of the RPI structure appear throughout history. In **engineering and architecture**, the classic project lifecycle separates analysis (requirements research) from design (planning) and construction (implementation). For example, traditional systems engineering uses phase-gated approaches where concept studies and requirements (research) precede design reviews (planning), which precede build/test (implementation). In **military strategy**, formal planning processes enforce reconnaissance and intelligence-gathering before drawing up battle plans, and only then issuing orders to act. The U.S. Army’s Military Decision-Making Process, for instance, requires a distinct *Mission Analysis* step to understand the situation and gather facts, followed by *Course of Action Development* (planning alternative strategies), before any execution order[\[2\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=Why%20it%20happens%3A%20Project%20teams,is%20a%20setup%20for%20failure). This ensures decisions are based on ground truth rather than gut instinct. Similarly, John Boyd’s famous **OODA loop** (Observe–Orient–Decide–Act) can be seen as an iterative microcosm of RPI: observing and orienting equate to research (gather facts, understand context), deciding corresponds to choosing a plan, and acting is implementation. The pattern even underpins business quality frameworks like the **Plan–Do–Check–Act (PDCA) cycle**, which emphasizes planning based on observations, then implementing, then evaluating results for continuous improvement[\[3\]](https://en.wikipedia.org/wiki/PDCA#:~:text=ImageThe%20plan%E2%80%93do%E2%80%93check%E2%80%93act%20cycle). RPI aligns with these frameworks’ emphasis on *no action without a plan, and no plan without facts*.

**Pattern vs. Checklist:** It is important to clarify why RPI is described as a *pattern* of thinking and execution rather than a prescriptive, linear workflow. As with the scientific method, the value of RPI lies in its adaptable principles, not in enforcing bureaucracy. Real-world problem solving is rarely a neatly ordered sequence – it is often messy, iterative, and creative. RPI’s phases should therefore be seen as **logically distinct modes or mindsets** rather than strict chronological stages. Research, Plan, and Implement will often overlap or cycle (e.g. implementation might reveal new facts requiring more research). What makes RPI a robust pattern is that it encourages *separation of concerns* – dedicating attention to one kind of thinking at a time – even if in practice teams may loop through RPI multiple times. This stands in contrast to treating RPI as a one-off “waterfall” process. As noted in discussions of the scientific method, the steps are not always in the same order nor all needed every time, but the *principles* remain applicable[\[1\]](https://en.wikipedia.org/wiki/Scientific_method#:~:text=While%20the%20scientific%20method%20is,have%20not%20followed%20the%20textbook). In essence, RPI offers a **reusable mental model**: whatever the domain, first ensure you truly understand the problem, then chart a course of action, then execute deliberately. Those principles can be instantiated in many workflows (from formal stage-gated projects to agile sprints), making RPI a pattern that underlies various methodologies without being tied to a specific one.

## Cognitive and Organizational Rationale for Separating R, P, and I

**Reducing Cognitive Load through Phase Separation:** Human cognition has well-known limitations in attention and working memory. By separating problem-solving into a research→plan→implement sequence, we reduce the mental load at each step. In the *Research* phase, individuals and teams focus exclusively on learning and analysis, without the pressure to generate solutions immediately. This compartmentalization helps avoid the strain of trying to **understand a complex problem at the same time as inventing a solution**. There is psychological evidence that multitasking or blending different modes of thinking can degrade quality – RPI counters this by encouraging a focused, sequential approach (first understand, then solve). In effect, it enforces a form of **System 2 thinking** (deliberative, analytical thought) by disallowing premature action[\[4\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=technical%20reality%20is%20well,process%20on%20the%20AI)[\[5\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,slow%2C%20deliberative%29%20thinking). The RPI workflow has even been described as forcing an AI or a team out of impulsive “System 1” behavior into a slower, reasoned mode[\[4\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=technical%20reality%20is%20well,process%20on%20the%20AI)[\[5\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,slow%2C%20deliberative%29%20thinking). This deliberate pace reduces mistakes that arise from cognitive overload or snap judgments.

**Avoiding Premature Convergence and Solutioneering:** One of the strongest rationales for a separate research phase is to combat “solutioneering” – the tendency to jump to a solution before the problem is fully understood. Humans are often *more comfortable with solutions than with problems*[\[6\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=Why%20We%20Don%E2%80%99t%20Value%20Understanding,the%20Problem). Psychologically, an undefined problem creates anxiety, whereas latching onto a specific solution (even an ill-founded one) provides a sense of relief and certainty[\[6\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=Why%20We%20Don%E2%80%99t%20Value%20Understanding,the%20Problem). Unfortunately, this comfort is illusory; teams that leap to implementation often find later that they solved the wrong problem or missed critical requirements[\[7\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=Over%20and%20over%20again%20I,teams%20do%20to%20break%20through)[\[8\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=So%20jumping%20to%20solutions%20is,bad%20for%20developing%20software%20products). The RPI pattern imposes a pause to articulate the problem and gather evidence, thereby **mitigating confirmation bias and premature consensus**. Research in design and product development has repeatedly shown that spending effort up front to understand “the problem behind the problem” leads to better outcomes[\[9\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=ask%20them%20a%20question%20that,%E2%80%9CWhat%20problem%20does%20this%20solve%3F%E2%80%9D)[\[8\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=So%20jumping%20to%20solutions%20is,bad%20for%20developing%20software%20products). By insisting on a research phase, RPI forces teams to confront uncomfortable questions early, rather than papering over them with a convenient solution hypothesis. This reduces the risk of **premature convergence** on a suboptimal design. It also counters individuals’ confirmation bias – in RPI, you must gather factual evidence and user insights that might challenge your initial assumptions (factual research), rather than cherry-picking data to support a pet idea.

**Counteracting Biases and Errors:** RPI’s structure can be seen as a safeguard against several cognitive biases: **confirmation bias**, **anchoring, and authority bias** among them. In organizations without a research phase, a common scenario is that a leader proposes a solution and the team, consciously or not, anchors around that idea. Any subsequent “research” often devolves into finding opinions or data that back the boss’s idea – a phenomenon sometimes called *“decision-based evidence-making”*, where evidence is retrofitted to justify a pre-made decision[\[10\]](https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false&triedRedirect=true#:~:text=Decision,that%20ultimately%20informs%20the%20decision). The RPI approach, by contrast, demands an impartial research step to establish ground truth (e.g. actual user needs, system constraints, empirical data) *before* a plan is chosen. This sequencing makes it harder to engage in motivated reasoning. If one tries to perform Research with an implementation already decided, it becomes obvious when the collected facts don’t fit – surfacing disconfirming evidence early. In other words, RPI encourages **evidence-based decision making instead of decision-based evidence**[\[10\]](https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false&triedRedirect=true#:~:text=Decision,that%20ultimately%20informs%20the%20decision). It structurally separates the evidence-gathering from the decision, reducing the influence of pre-formed opinions. Additionally, by including a distinct Plan phase, RPI creates a natural point for **peer review and dissent**. Teams are prompted to review the plan (design) before execution, which allows challenging assumptions and spotting logical flaws when it is still cheap to make changes. This is analogous to Atul Gawande’s **surgical checklist** concept: by pausing to verify that the plan meets objective criteria (e.g. all tasks feasible, risks addressed) before cutting, errors and “blind spots” due to bias can be caught[\[11\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=where%20RPI%20is%20most%20needed,validated%20process). Overall, the RPI pattern acts as a form of *cognitive guardrail*, compelling teams to slow down and apply critical thinking at each phase, thereby reducing errors from biases like overconfidence and tunnel vision.

**Preventing Rework and “Firefighting” Mode:** On an organizational level, a major rationale for RPI is reducing costly rework and thrash. Skipping proper research or planning often leads to what engineers grimly call “**shoot-then-aim**” development. The team rushes into implementation, only to discover mid-stream (or worse, after release) that requirements were misunderstood or the solution approach was flawed. They then must scramble to fix or redo work, incurring delays and technical debt. Empirical project analyses have found poor upfront requirements and planning to be leading causes of project failure or overruns[\[2\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=Why%20it%20happens%3A%20Project%20teams,is%20a%20setup%20for%20failure)[\[12\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=4). By contrast, an RPI approach front-loads the discovery of unknowns and clarification of scope. It explicitly allocates time to reduce ambiguity (in Research) and to map out the work (in Plan). This **investment in clarity** tends to pay off by minimizing expensive changes later. RPI thereby tackles the common organizational pathology of *perpetual firefighting*, where teams are always reacting to problems that proper planning could have foreseen. Instead of oscillating between crisis and patchwork fixes, an RPI-driven culture emphasizes thoughtful preparation followed by smoother execution. In practical terms, this means fewer surprise setbacks during implementation and less ambiguity-induced churn (e.g. design changes, scrapped code, scope creep). The cognitive principle here is simple: *it is cheaper and easier to solve problems in your head or on paper than in code or in production*. RPI forces more of the problem to be solved mentally (when changes are free) and less during hands-on execution (when changes are costly).

**Maintaining Focus and Avoiding “Context Switching” Waste:** Separating research, plan, and implement also has benefits for team coordination and focus. In knowledge work, constantly switching between different modes – say, brainstorming one minute, coding the next, then back to research – incurs heavy context-switching penalties. By grouping activities, RPI allows teams to get “in the zone” for each type of work. During Research, the mindset is exploratory and divergent; during Plan, it is evaluative and convergent; during Implement, it is procedural and productive. These demand different cognitive skills and even different tempos. An RPI structure lets teams **commit fully to one mode at a time**. For example, engineers can spend a dedicated period reading documentation, analyzing data, or asking questions (Research) without the pressure to also produce code at that moment. Later, in the Implement phase, they can focus on execution details knowing that higher-level questions have been answered in the plan. This improves quality by minimizing the mental juggling of unresolved questions during coding. Organizationally, it can also reduce thrash: meetings and reviews can be aligned to phase transitions (e.g. a design review after Plan) rather than interrupting daily implementation work with fundamental questions that should have been settled earlier. In summary, RPI optimizes **cognitive flow** by aligning the team’s state of mind with the task at hand, thereby reducing distraction and confusion.

## Boundaries and Non-Goals of RPI

Not every scenario warrants a full RPI cycle, and over-applying the pattern can be counterproductive. It’s critical to delineate what RPI *is not* and where it offers diminishing returns:

**Not a Replacement for Agility or Iteration:** RPI is sometimes misconstrued as a strictly sequential “waterfall” process that conflicts with iterative methods. In truth, RPI and iteration operate on different axes: RPI focuses on *thought structure within each cycle*, whereas iteration concerns *the repetition of cycles*. The pattern does **not** mean one must fully research and plan everything upfront for a multi-year project with no learning loops. Rather, you can execute RPI in **small increments** – for example, each sprint or each feature can go through a mini research/plan phase before coding. RPI is compatible with agile development when used at the right granularity. What RPI is *not* is an endorsement of analysis-paralysis or indefinite up-front design with no feedback. It doesn’t advocate big-design-up-front for its own sake; it advocates *appropriate* design up front for the problem at hand. In agile terms, RPI simply ensures that even in rapid iterations, there is a moment of *problem understanding* and *solution formulation* before jumping into user stories or coding. It pushes back on a misinterpretation of “agile” that sometimes occurs – teams skipping research/design entirely in the name of speed. RPI insists that **haste makes waste**: even in fast cycles, spending a bit of time to think can avoid cycles of refactoring later[\[2\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=Why%20it%20happens%3A%20Project%20teams,is%20a%20setup%20for%20failure). That said, RPI does *not* require that all uncertainty be resolved before implementation begins; it acknowledges diminishing returns of analysis, especially in complex adaptive problems where only iteration reveals truth. Hence an RPI-informed approach might plan to implement **probes or experiments** as part of learning, rather than to lock down a complete solution blueprint on day one.

**Not Rigid Ritual or Documentation for Show:** A key non-goal of RPI is to avoid process for process’ sake. The value of the pattern lies in its *outcomes* (reduced ambiguity and error), not in merely producing deliverables (research reports, plan documents) to satisfy bureaucracy. RPI should not degenerate into what one engineer termed “**documentation theater**,” where teams create elaborate plans or research docs that no one reads or uses, just to tick a box[\[13\]](https://passo.uno/ai-wikis-docs-teather-as-a-service/#:~:text=,because%20that%E2%80%99s%20The%20Right%20Way%E2%84%A2). Documentation theater occurs when planning is done to appease management or follow a template, rather than to truly inform implementation[\[13\]](https://passo.uno/ai-wikis-docs-teather-as-a-service/#:~:text=,because%20that%E2%80%99s%20The%20Right%20Way%E2%84%A2). RPI explicitly guards against this by tying each phase to actionable outputs and decisions. For example, the end of Research should yield concrete *evidence or insights* that drive the Plan; the Plan should yield a *working breakdown* that directly guides implementation tasks. If these are missing, then the artifacts are just performative. In many “waterfall” antipatterns, teams spent weeks writing exhaustive specification documents (Plan) that quickly became outdated and were never referenced during actual development – a pure documentation theater scenario. RPI’s intent is the opposite: it prefers a one-page relevant plan over a 100-page shelf document. In fact, one can view RPI as a *lean* approach to upfront work – do just enough research to identify key facts, just enough planning to decide and coordinate, then implement and iterate. When a plan becomes **“planning theater” (documentation as an end in itself)**, it violates RPI’s principles. Thus, RPI is *not* about generating paperwork or meeting checklists; it’s about thinking. If the team already has the necessary context or alignment, RPI phases can be extremely lightweight (even implicit). Skipping writing a formal document is not skipping RPI – skipping the thinking is.

**When RPI Is Overkill:** There are scenarios where a full RPI cycle provides little benefit, and enforcing it would slow progress needlessly. For **small, well-understood problems**, formal research and planning may be unnecessary. For example, if a developer needs to fix a minor bug or implement a trivial feature, they often can (and should) just do it directly. The pattern still applies in a trivial sense (they quickly *check context*, *decide* how to fix, *code* it), but there is no need for an explicit phase separation or documentation. RPI shines for **complex or novel problems** where the risk of misunderstanding or wrong approaches is high. Conversely, for repetitive tasks with known solutions, insisting on a drawn-out RPI process could be counterproductive. Another context where RPI might be less appropriate is in **highly exploratory creative work**. For instance, in early product innovation or pure research, the goal is to discover the problem space through rapid experimentation (the ethos of lean startup’s *build–measure–learn*). Here, too much upfront structuring (research/planning) can stifle creativity or lead to analysis paralysis. RPI is not a panacea – if the cost of a wrong hypothesis is low and learning is cheap, sometimes diving in and iterating (skipping deep initial research) can be acceptable or even preferred[\[14\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Argues%20for%20the%20power%20of,loading%20advocated%20here). An example is **user interface A/B testing**: rather than theorize extensively which design is better (research/plan), it can be faster to implement two variants and see actual user behavior (direct experiment). RPI would caution to at least form a hypothesis about the user need first, but it wouldn’t demand a full research study for something that can be empirically decided quickly. In summary, **RPI is not always necessary**, particularly when uncertainty is best resolved by quick trial-and-error or when the domain is simple. The pattern’s formality should scale to the problem – rigidly doing lengthy R, P for every tiny change is a misuse.

**Contrast with Execution-First Cultures:** RPI stands in opposition to “just execute” cultures where planning and research are culturally devalued. Some fast-paced startups or teams pride themselves on a *hustle mentality* of shipping features quickly and figuring things out on the fly. While this can yield speed, it often does so at the expense of quality and long-term sustainability. RPI explicitly defines boundaries against such execution-first mindset: it asserts that **starting implementation without clarity** is courting disaster. A useful contrast is with environments where **agile rituals become thinking shortcuts**. For example, teams might rely on backlogs and daily stand-ups as a veneer of process, but in reality they are continually reacting and coding based on superficial understanding (“vibe coding” as some have called it[\[15\]](https://www.reddit.com/r/vibecoding/comments/1qs80k4/everything_one_should_know_about_specdriven/#:~:text=Software%20development%20is%20moving%20fast%2C,that%20it%20lacks%20solid%20planning)). Agile ceremonies cannot substitute for actual research or design; they are meant to facilitate adaptation *once those inputs are done*. RPI’s boundary here is clear: Scrum, Kanban, etc., manage *how* work moves, but RPI addresses *how we think about the work*. If an organization is using iteration as an excuse to never do homework (“why research, we’ll just iterate until it feels right”), RPI would be a necessary corrective. However, RPI is *not* anti-iteration – it just insists that each iteration be informed by thought. It draws a line against the extreme end of “move fast and break things” where moving fast becomes an excuse for not understanding what or why we’re breaking. In such execution-first cultures, typical failure modes include accumulating massive technical debt, delivering features that users don’t need (solving the wrong problems), or burning out teams with constant rework. RPI is deliberately *counter-cultural* in those settings: it introduces a pause for reflection that may feel “slow” but prevents the far slower process of repeated failure.

**Explicit Non-Goals:** To summarize what RPI is not aimed at: it is not about slavishly following a formal process regardless of context; it is not opposed to change or iteration (in fact, it expects to cycle RPI as needed); it is not about producing extensive documentation or stage-gates to please management; and it is certainly not about inflexible one-size-fits-all execution. When used appropriately, RPI is a means to achieve clarity and correctness *without* succumbing to bureaucracy or paralysis. It must be applied with judgment – knowing when to compress phases, when to iterate quickly, and when to stick to the rigor. The next sections on failure modes will further illustrate what happens when RPI’s intent is misunderstood or misapplied.

## Common Anti-Patterns and Failure Modes in RPI Adoption

Even with the best intentions, organizations and individuals can fall into traps that subvert the Research–Plan–Implement pattern. Below is a **taxonomy of common anti-patterns**, each describing a way RPI can fail or be superficially adopted. For each, we examine how the anti-pattern emerges, why it is tempting, and the consequences it brings:

### 1\. **Collapsing Research into Opinion Gathering**

In this anti-pattern, the “Research” phase exists in name but not in substance – instead of gathering facts and evidence, the team merely gathers **opinions or anecdotal input**. This often emerges in organizations that lack rigor in analysis or have weak information practices. For example, a product team might conduct “research” by holding a few brainstorming meetings, soliciting unvetted ideas from stakeholders, or cherry-picking user feedback that supports a preconceived notion. No systematic fact-finding (such as data analysis, experiments, literature review, or root-cause investigation) actually takes place. The result is a false sense of knowledge: the team feels they’ve done due diligence because they heard lots of voices, but they may just have an echo chamber of biases. One root cause of this anti-pattern is **confirmation bias** – teams unconsciously seek opinions that validate their initial beliefs. Another cause is organizational habit: some cultures equate consensus with truth, so “research” becomes asking around until everyone agrees, rather than checking reality. This pattern is attractive because it’s *easy* and *fast*. Gathering opinions (especially from higher-ups or loud voices) gives a comforting feeling of having support, and it avoids the hard work of analysis. It also flatters leaders – the HiPPO effect (Highest Paid Person’s Opinion) often drives this: if the boss or a respected expert says it, that substitutes for independent research[\[16\]](https://www.benchmarksixsigma.com/forum/topic/36154-hippo-effect/#:~:text=HiPPO%20Effect%20is%20the%20effect,opinions%20influence%20the%20group%27s%20discussion)[\[17\]](https://www.benchmarksixsigma.com/forum/topic/36154-hippo-effect/#:~:text=HIPPO%20is%20one%20of%20the,has%20a%20very%20high%20pay). The damage, however, is severe. When research collapses into opinion, decisions are essentially made on unverified assumptions or biases. The team might confidently proceed to Plan and Implement solutions that have no factual basis. This leads to **solving the wrong problem** or building on a foundation of sand. Moreover, it can entrench authority bias – if critical data later contradicts the chosen course, it may be ignored because “our research (i.e. our consensus) says otherwise.” In effect, this anti-pattern nullifies the purpose of RPI’s first phase. A tell-tale sign is when documentation of the “Research” phase contains plenty of bullet-pointed opinions (“Stakeholder X believes users want Y…”) but scant evidence or references. To recover from this anti-pattern, teams need to instill more rigorous information gathering: e.g. user studies with actual data, competitive analysis, technical prototypes to get facts, etc., and treat opinions as hypotheses to test, not conclusions. RPI only works when “Research” yields *objective, relevant* insights – anything else is just noise.

### 2\. **Plan as “Documentation Theater”**

This anti-pattern occurs when the Planning phase produces extensive documentation or plans that are performative and not truly actionable. The term *documentation theater*[\[13\]](https://passo.uno/ai-wikis-docs-teather-as-a-service/#:~:text=,because%20that%E2%80%99s%20The%20Right%20Way%E2%84%A2) captures it well: the team invests time in creating the appearance of a detailed plan (often to satisfy process checkpoints or management expectations) but with no intent or mechanism to use that plan during implementation. This emerges in organizations where process compliance is emphasized over outcomes, or where there is a fear of proceeding without a hefty document to waive at stakeholders. Often the plan is a large specification, design document, or Gantt chart produced in a rush to meet a deadline (“we must deliver a plan by Q1”). It’s attractive because it gives a **sense of security and accomplishment** – both to planners and to stakeholders who see a thick document and assume the project is well in hand. It can also be driven by inertia: perhaps a template exists from a PMO, so the team fills in sections of a Plan document religiously, even if those sections add no new insight. The major issue is that such plans are **divorced from reality**. They might be outdated the moment they are written, overly generic, or filled with platitudes and boilerplate. In worst cases, it’s a “PowerPoint plan” – slides showing ambitious timelines and checkboxes that look nice but haven’t grappled with the actual complexities. Why is this damaging? Because it **wastes effort and misleads decision-makers**. The team believes they have a plan, so they charge ahead, only to discover later that the plan was incomplete or unworkable (since it was never critically reviewed or validated). Meanwhile, stakeholders might allocate resources based on a fictional roadmap. This leads to chaos in implementation: scope creep, constant change requests, or paralysis when the team realizes the plan doesn’t match reality. Another harm is demoralization – engineers quickly become cynical if they know planning is just an exercise in filling out documents that will be ignored. They will either disengage from planning (reinforcing the execution-first culture) or follow the flawed plan blindly to “check the box,” resulting in poor outcomes. *Documentation theater* usually goes hand-in-hand with lack of meaningful review. In a healthy RPI, the Plan should be scrutinized and tested (does it address the research findings? Is it feasible with resources? What are the known risks?). In this anti-pattern, the plan may not be critically examined – or feedback is superficial, focusing on format over substance. To avoid this failure mode, organizations should emphasize **quality over quantity** in planning. A concise plan that clearly identifies tasks, owners, design decisions, and success criteria is far better than a verbose document nobody reads. Using checklists like the FACTS criteria (Feasible, Atomic, Clear, Testable, Scoped)[\[18\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Decide%20What%20to%20Do,Clear%2C%20Testable%2C%20and%20properly%20Scoped) for plan tasks can help ensure that a plan is actionable rather than theatrical. In essence, *Planning should be a thinking activity, not a writing-only activity*. When teams find themselves doing “planning” just to satisfy a process (e.g. writing a 50-page design because it’s required), they should pause and refocus on what decisions and clarity the plan needs to provide to implementation. If it’s not providing that, it’s just theater.

### 3\. **Jumping to Implement (Execution Pressure and Authority Bias)**

This is perhaps the classic failure mode RPI was created to prevent: the team skips or truncates the Research and Plan phases and dives straight into Implementation. Despite RPI being known, real-world pressures can cause this anti-pattern to surface frequently. Two big drivers are **time pressure** (“we have an impossible deadline, just start coding now\!”) and **authority or hierarchy** (“the boss/product owner already decided what to do; no need for further research or discussion”). In such cases, even if a team intends to follow RPI, they may be told implicitly or explicitly to “just get started”. Stakeholders might perceive research and planning as overhead that slow down delivery, especially when short-term results are demanded. This anti-pattern is very attractive to those outside the implementation team: from a distance, *coding or building feels like progress*, whereas researching or planning can be invisible work. Thus managers under deadline stress often push teams to cut the “pre-work” and begin execution immediately[\[2\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=Why%20it%20happens%3A%20Project%20teams,is%20a%20setup%20for%20failure). Additionally, in top-down cultures, if a leader believes they already know the solution, they may see research as redundant and planning as mere formalizing – so they push directly into execution with the predetermined solution (a combination of authority bias and overconfidence). The damage from jumping straight to Implement is well-documented in project post-mortems. Without adequate research, the team is likely solving ill-understood problems, leading to **massive rework** later when reality intrudes. Without a clear plan, coordination suffers: people make conflicting changes, overlook critical tasks (like testing, security, integration), or build something that doesn’t align with user needs. A PMI study famously found poor requirements management (a research/planning activity) to be a top cause of project failure[\[19\]](https://www.pmi.org/learning/library/poor-requirements-management-source-failed-projects-9341#:~:text=I%20still%20don%27t%20have%20time,In). In code terms, jumping to implementation often yields “spaghetti” or technical debt, as architectural considerations are discovered too late. Moreover, skipping R and P means skipping risk analysis – so high risks turn into issues that blindside the project mid-course. This anti-pattern also *amplifies authority bias* problems: if implementation is rushed because a powerful stakeholder insisted on a particular solution, it becomes very hard to later course-correct or question that solution (sunk cost and political risk). The team might continue on a doomed trajectory far longer because “we’re already building it” and momentum takes over. In a human–AI collaboration context, jumping to implement is akin to prompting an AI to execute a task with no prior reasoning – the result is typically shallow and error-prone[\[20\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Q%3A%20How%20does%20the%20Research,workflow%20prevent%20slop). To avoid this anti-pattern, organizations should enforce **gating criteria**: for instance, refusing to allow coding to start until a minimal research and design artifact is reviewed and approved. One practical measure is requiring a lightweight *“start-of-work” checklist* (did we identify the problem? Do we have success criteria? Did we break the work into tasks?). This aligns with site reliability principles at companies like Google that note manual review steps can be seen as bottlenecks but are crucial for quality[\[21\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=However%2C%20the%20approach%20heavily%20relies,to%20avoid%20semantic%20fatigue). While deadlines are real, teams must communicate that **speed without direction is folly** – a short delay to formulate a plan saves significant time in execution[\[2\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=Why%20it%20happens%3A%20Project%20teams,is%20a%20setup%20for%20failure).

### 4\. **Performing “Research” with the Implementation Already Decided**

In this subtle failure mode, the sequence of RPI is formally followed, but in reality the implementation decision was made beforehand (often by leadership or a dominant team member). The research phase thus becomes an exercise in **confirmation bias**: finding data to justify the chosen implementation, and filtering out data that contradicts it[\[10\]](https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false&triedRedirect=true#:~:text=Decision,that%20ultimately%20informs%20the%20decision). This is sometimes called *decision-based evidence making* – the decision is made first, evidence is collected second[\[10\]](https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false&triedRedirect=true#:~:text=Decision,that%20ultimately%20informs%20the%20decision). It emerges in situations where a pet project or technology is favored by someone in power, or where political or emotional investment in a particular solution is high. For example, a CEO might have decided “We need to re-build our platform in Microservice X” due to an industry trend. The team is then tasked to “research the best way” – but only research that aligns with the CEO’s mandate will be accepted. Alternatively, an engineering group may be enamored with a tool or approach and unconsciously shape their analysis to support using it. This anti-pattern is attractive because it gives the illusion of due diligence while effectively allowing the team (or leader) to do what they wanted to do all along. It’s a convenient way to handle RPI in organizations that expect some process: you perform the ceremony of research, but steer it to the predetermined outcome. It’s psychologically comfortable too – nobody has to grapple with the possibility that their favored solution is wrong, since the “research” conveniently supports it. The damage of this approach is that it gut-renovates the purpose of RPI. Instead of reducing bias and error, it **institutionalizes bias** under the guise of process. The team may ignore glaring red flags that the chosen implementation is flawed, because acknowledging them would violate the script. When reality doesn’t cooperate (say, the data clearly shows the customers don’t want the proposed feature), such organizations often rationalize it away or bury the findings. Ultimately this leads to failed projects or wasted investments – building something that wasn’t truly validated. It can also destroy trust: team members see that research is just theater and become disillusioned, and stakeholders eventually see that decisions aren’t actually evidence-based despite claims. A notorious real-world example was the lead-up to certain public policy decisions where intelligence was “fixed around the policy” – the evidence was selectively gathered to justify an already-decided course[\[22\]](https://nsarchive2.gwu.edu/NSAEBB/NSAEBB328/II-Doc14.pdf#:~:text=But%20the%20intelligence%20and%20facts,for%20publishing%20material%20on). In technology projects, this might manifest as pushing through a solution that all unbiased analysis would question, simply because someone’s ego or career is tied to it. The remedy is *genuine intellectual honesty* in the Research phase. One should explicitly ask: “What evidence could prove our favored idea is wrong?” and then seek it. Peer review can help – involving independent reviewers or skeptics in the research review can surface bias. Leadership must also practice humility: if research comes back negating the initial idea, they should be willing to pivot the plan. In short, this anti-pattern is a failure of leadership and culture as much as process – it requires a commitment to **evidence-based decision making** as a value. RPI only yields its benefits if the “R” is allowed to sometimes derail or alter the “I”. If the implementation is immutable from the start, you are effectively doing a biased Plan–Implement and skipping real Research.

### 5\. **Infinite Research with No Commitment (Analysis Paralysis)**

At the opposite extreme of skipping research is the anti-pattern of **never finishing research (or planning) at all**. Here, the team keeps spinning in the research phase – investigating, collecting data, analyzing – or in the planning phase – drafting and redrafting plans – and repeatedly postpones implementation because “we aren’t ready yet.” This often stems from fear: fear of making a wrong decision, of uncertainty, or of stakeholder blame if something goes wrong. It can also come from perfectionism in analytical roles – the belief that *just a little more data will remove all uncertainty*. This anti-pattern is common in risk-averse organizational cultures or among teams tackling very complex problems with no clear end to “knowing enough.” It might start with good intentions (thoroughness), but lacks a mechanism to declare “enough is enough.” The attractiveness of infinite research is that it **delays commitment** – as long as you’re researching, you haven’t failed yet. It provides a comforting illusion that with just more analysis, you can eliminate risk entirely. Some individuals also simply enjoy research (or planning) and feel less comfortable in execution, so they subconsciously prolong the phase they excel in. The damage from analysis paralysis is twofold: *opportunity cost* and *morale impact*. While the team churns in pre-implementation activities, the external world moves on – windows of market opportunity can close, users remain with an unsolved problem, or competitors act. Internally, excessive analysis yields sharply diminishing returns[\[23\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=In%20the%20IT%20management%20context%2C,Per%20Wikipedia) – after a point, new data or increasingly granular plans are not adding real value to decisions[\[23\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=In%20the%20IT%20management%20context%2C,Per%20Wikipedia). Instead, they may even muddy the waters or lead to overengineering. Projects stuck in this limbo often miss their timing, or end up implementing something obsolete because the ground shifted during their analysis. Morale-wise, teams can grow frustrated or cynical: the “real work” never starts, and they may doubt leadership’s ability to execute. It can also burn out analysts as they keep preparing elaborate reports that lead nowhere. Furthermore, **groupthink and false security** can arise: with so much analysis, teams might develop a false confidence in their knowledge and be less adaptive when they finally implement and encounter surprises. In extreme cases, the project is canceled after burning time and money with nothing to show – a classic failure. Objectively, signs of this anti-pattern include repeatedly extending the research timeline, constantly changing requirements or scope from new findings, and lack of concrete progress markers. Preventing analysis paralysis requires setting **clear exit criteria and limits** for the upfront phases. For instance, management can impose timeboxes: “We will spend 4 weeks on research, no more, then make a decision.” Or define what questions must be answered to proceed, and once answered, force a go/no-go. As one IT management guide puts it, analysis must serve a specific purpose and have defined deliverables; open-ended analysis without decision points becomes “empty and excessive”[\[24\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=In%20the%20IT%20management%20context%2C,Per%20Wikipedia)[\[25\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=,point%20at%20which%20decisions%20could). Introducing stage gates or milestone reviews can enforce that planning stops at a reasonable point and triggers implementation. Another tactic is prototyping: if uncertainty is high, instead of analyzing endlessly, do a small implementation experiment to gather real data (thus transitioning out of pure research). This is effectively using implementation *as a form of research* when appropriate, to break the paralysis. Finally, leadership should foster a culture that accepts that no plan is perfect – *some* risk must be taken. That means people won’t be punished for unforeseeable issues, as long as due diligence was done. When teams feel psychologically safe to proceed with partial information, they are more likely to draw a line and start implementing when the marginal benefit of more research drops. In sum, RPI requires balance: enough research to be confident, but not so much that you never act.

These anti-patterns illustrate that merely knowing the names “Research, Plan, Implement” is not enough – **how** those phases are executed and respected is critical. Each failure mode above is essentially a distortion: doing RPI in form but not in spirit (e.g. \#1, \#4), or doing it incompletely (\#3), or overdoing a part (\#5). Avoiding these pitfalls requires vigilance, good organizational habits, and often, explicit checks (like validation criteria, peer reviews, or management edicts as described) to keep the RPI engine running smoothly.

## RPI in Practice Across Different Domains

The RPI pattern manifests in various fields with domain-specific adaptations while preserving its core logic. Let’s examine how Research–Plan–Implement appears in a few areas: **software engineering, product design, organizational change, public policy**, and **AI-assisted workflows**. Each domain emphasizes different techniques in each phase, but the structural separation of understanding, planning, and executing is consistently beneficial.

### Software Engineering

In software development, the RPI pattern can be recognized in processes ranging from classical models to modern AI-driven coding. Historically, the **waterfall model** explicitly had requirements analysis (research), design (plan), implementation (code), followed by testing and maintenance. Agile methodologies were a reaction to waterfall’s rigidity, but they did not eliminate the need for analysis and design – they merely iterate on smaller chunks. A healthy software team still performs RPI, just at the feature or sprint level. For instance, when tackling a complex feature in a legacy codebase, developers should first do research: read existing code, understand user stories, clarify requirements. Skipping this often leads to the notorious “*vibe coding*” scenario[\[15\]](https://www.reddit.com/r/vibecoding/comments/1qs80k4/everything_one_should_know_about_specdriven/#:~:text=Software%20development%20is%20moving%20fast%2C,that%20it%20lacks%20solid%20planning), where the AI or developer just writes code that “feels” right but breaks things (context drift, regressions, etc.). In fact, the rise of AI coding assistants has *re-emphasized* RPI in software. As one analysis notes, naive use of AI coding tools leads to a lot of “slop code” that later needs rework[\[26\]](https://medium.com/aiguys/no-vibes-allowed-solving-hard-problems-in-complex-codebases-d550d165863e#:~:text=It%20seems%20pretty%20well,that%20was%20shipped%20last%20week). To counter this, advanced teams use RPI-like workflows: have the AI/engineer *research the codebase and problem* (e.g. asking the AI to read relevant files and ask clarifying questions), then *plan the code changes* (perhaps having the AI outline step-by-step changes or a spec), then *implement* systematically[\[27\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=%EF%B8%8F%20The%20RPI%20Workflow%20,Plan%2C%20Implement)[\[4\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=technical%20reality%20is%20well,process%20on%20the%20AI). This approach has been shown to let AI handle even large, brownfield codebases successfully by enforcing context and forethought[\[28\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,risk%20of%20errors%20or%20hallucinations)[\[4\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=technical%20reality%20is%20well,process%20on%20the%20AI). In simpler terms, it operationalizes the principle that even in coding: **don’t code without a plan, and don’t plan without understanding the code**. Notably, the RPI pattern in software is essentially a return to **spec-driven development** values (writing a specification or plan that the code must follow), but updated for modern continuous workflows[\[29\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=plans%20,to%20avoid%20semantic%20fatigue). The difference is that agile RPI is lightweight and iterative – maybe the “plan” is just a checklist of tasks or a user story acceptance criteria, not a huge design document, but it’s still a plan. The key structural adaptation in software is the integration of testing/feedback: often RPI in coding is extended to RPI-T (Test) cycles or similar, ensuring the Implement phase includes verification (as suggested by the PDCA cycle’s Check). Many teams enforce that each implemented chunk must pass tests and code review, effectively folding a mini-“check” into the I phase[\[30\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Implement%3A%20With%20your%20plan%20doc,creep%E2%80%94just%20focused%20execution%20that%20works). All told, software engineering demonstrates that RPI can scale from the micro (fixing a bug with a quick read-plan-code) to the macro (architecting a system), and that including R and P phases explicitly leads to more robust, maintainable code bases[\[31\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Q%3A%20How%20does%20the%20Research,workflow%20prevent%20slop). When software organizations ignore this (for example, a culture of “just keep coding and we’ll refactor later”), they often suffer from high defect rates and inability to evolve the system coherently.

### Product and Service Design

In user-centered product design (encompassing UX design, industrial design, etc.), the RPI pattern is foundational. Design thinking frameworks are essentially elaborate RPI cycles: **Empathize/Research**, **Define (the problem)**, **Ideate/Plan solutions**, **Prototype & Test (Implement and evaluate)**[\[32\]](https://www.ama.org/marketing-news/the-5-phases-of-design-thinking/#:~:text=5%20Phases%20of%20Design%20Thinking,Define%2C%20Ideate%2C%20Prototype%2C%20and%20Test). In practice, product teams spend significant effort on the initial research: user interviews, market analysis, studying usage data – all to ensure they are solving a real problem and to uncover user needs. This is formalized in tools like user personas and journey maps (outputs of the Research phase that inform planning). A known industry saying “**fall in love with the problem, not the solution**” encourages teams to fully understand the problem (R) before brainstorming solutions (P). The “Plan” in design often takes the form of sketching, wireframing, or storyboarding multiple ideas and then selecting the best via design reviews – it’s a planning process for the creative solution. Only then do designers build prototypes or MVPs (Implement) to validate assumptions. RPI helps avoid what designers call *“jumping to solutions”* or *“feature-itis”*. As an example, a team might be tempted to add a bunch of features to an app to satisfy perceived needs; an RPI approach would have them step back and research what users’ core pain points are (maybe half the features aren’t needed, but something else is). By planning a coherent minimal solution to the right problem, the implementation (be it a UI prototype or a service launch) is far more likely to succeed with users. In contrast, when RPI is not followed, companies often build products loaded with poorly understood features – leading to low adoption or the need to pivot. Another adaptation in design is the concept of **iteration between phases**: after implementing a prototype, designers gather feedback (new research) and refine the plan. This is essentially repeating RPI in cycles known as **design iterations**. The Double Diamond model in design (Discover → Define → Develop → Deliver) explicitly separates a divergent research phase and convergent define phase (which map to R and P) before development and delivery (I). RPI also underscores avoiding solution bias in research: good UX research strives to ask open-ended questions rather than leading ones, to truly hear user problems rather than seeking validation for the team’s ideas. This echoes the earlier anti-pattern of not collapsing research into opinion – in design, that would be like only listening to internal opinions without observing real user behavior. Mature product organizations thus invest heavily in the Research phase (user research is a whole profession) and have formal **design planning** stages (like design sprints) before engineers start coding. The core pattern remains: understand the user and context, plan the experience, then implement the product. Skipping these steps yields products that might technically work but fail to resonate or solve the right problem – a costly outcome in terms of wasted development and market failure.

### Organizational Change and Strategy

When implementing organizational change or formulating strategy, RPI often takes the form of **action research and planning cycles**. A classic approach in change management is: *research the current state (diagnose issues, gather data on processes, culture, etc.), plan the change (strategy, interventions, training programs), implement the change (roll out new policies, structures), then evaluate and adjust.* In fact, experts describe good change management as *“action research – plan, implement the first phase, check if it’s working, modify if not, then do the next phase”*[\[33\]](https://www.aicd.com.au/company-policies/crisis-management/plan/the-key-role-of-effective-change-adaptability-in-crisis-management.html#:~:text=Similarly%2C%20the%20delivery%20of%20any,%E2%80%9D). Here the RPI pattern might be repeated phase by phase, allowing adaptation. Consider a company undergoing a digital transformation. In the R (research) phase, they might survey employee skills, map out existing processes, identify pain points. If they collapse this step, the risk is high that they misjudge what needs changing (leading to resistance or failure). The P (plan) phase could involve setting a vision, defining initiatives (e.g. training programs, new IT systems), assigning roles, and establishing metrics. Without this, change efforts often flounder due to lack of coherence. The I (implement) phase is executing those initiatives – but importantly, with monitoring in place. A common failure mode in change initiatives is declaring victory too soon or not monitoring; RPI thinking would emphasize implementing *with* feedback loops (which essentially become the research for the next iteration). The adaptation in this domain is that implementation is often not a one-time rollout but continuous adjustment, so the boundaries between phases blur a bit: you implement some changes, then observe (research) results, then refine plan. However, even within an iterative change program, it’s vital not to skip straight to doing things without a research base. An example: a new CEO might feel urgency and start firing people or restructuring (implement) without fully understanding the company’s real problems – that often ends poorly. A better approach uses RPI: conduct an organizational assessment (maybe with external consultants for objectivity), then plan changes addressing root causes, then execute them while communicating and measuring. Also, RPI helps counter an **execution-first culture in management** where action is valued over thought. Many boards demand “quick wins” in change programs; RPI would counsel that any quick win still needs to be grounded in a researched understanding of what win is valuable and a plan that doesn’t cause collateral damage. In summary, successful organizational change uses RPI to avoid knee-jerk changes (e.g. copying a competitor’s structure without analysis) and to ensure that interventions are evidence-based. The US Army’s After Action Review mantra “Plan, Execute, Assess” similarly highlights that even in fluid situations, taking time to plan and later assess is crucial to organizational learning. RPI formalizes that logic in civilian organizations which might otherwise either procrastinate (too much analysis, no action) or thrash (action with no analysis). The balanced approach yields adaptive yet deliberate change.

### Public Policy and Governance

Policy making follows an RPI-like cycle often known as the **policy cycle**: *agenda-setting and research → policy formulation (planning) → implementation → evaluation*. In government, one sees a strong separation between analysis teams (think tanks, research arms) and the people who draft policy (planners) and those who execute or enforce policy (implementers). A well-known failure mode in policy is when laws are rushed through without sufficient research (e.g. not consulting data or experts) – this can lead to ineffective or harmful policies that must be retracted or that have unintended consequences. On the other hand, too much deliberation (infinite research) can stall needed reforms indefinitely. The RPI pattern is recognized as vital to evidence-based policymaking: for example, using pilot programs or committees to study an issue before committing to nationwide implementation (that’s research feeding into planning). A concrete adaptation in policy is the use of **impact assessments** and **white papers** – these are essentially research and plan documents respectively. An impact assessment (research) collects evidence on the likely effects of a proposed regulation. Only after reviewing it does the government finalize the policy design (plan) and then implement via legislation or programs. If policymakers skip the assessment, they risk passing laws that don’t address the real problem or that create new problems. Conversely, policy cycles emphasize **monitoring and evaluation** post-implementation – effectively adding an explicit check phase to feed back into the next cycle[\[33\]](https://www.aicd.com.au/company-policies/crisis-management/plan/the-key-role-of-effective-change-adaptability-in-crisis-management.html#:~:text=Similarly%2C%20the%20delivery%20of%20any,%E2%80%9D). This is akin to turning RPI into RPIE (Research, Plan, Implement, Evaluate) as a continuous loop. A successful example of RPI in policy: public health interventions often start with research (epidemiological data, pilot studies), then planning (policy design, resource allocation), implementation (rolling out a campaign or regulation), and then measuring health outcomes. When any step is shortchanged, outcomes suffer. Policy also illustrates the importance of RPI boundaries: sometimes political expediency forces skipping research (policy based on ideology or populism rather than facts), which is analogous to jumping to implement under authority bias – the result can be ineffective governance or scandals. Similarly, endless blue-ribbon commissions that never result in action exemplify analysis paralysis in policy. Thus, effective governance strives to follow RPI with discipline: research thoroughly but within time limits, plan with stakeholder input and clear objectives, implement decisively, then measure results and iterate. Modern trends like **evidence-based policy** and **policy labs** are institutional attempts to enforce RPI: they embed data analysis (R) in policy development and require structured trials (I) and evaluations. Essentially, while the language differs (problem definition, option analysis, etc.), the skeleton is RPI.

### AI-Assisted Workflows and Human–AI Collaboration

With the advent of AI agents and copilots, RPI has found a new and crucial application: making human–AI collaboration **reliable and safe**. AI systems, especially large language model-based agents, have a tendency to produce errors when allowed to act directly on tasks without deliberation – they may hallucinate facts, misunderstand instructions, or perform actions that don’t achieve the human’s true goal[\[31\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Q%3A%20How%20does%20the%20Research,workflow%20prevent%20slop). By imposing an RPI structure on AI workflows, we align the AI’s process with a more human-like problem-solving rigor. For example, instead of prompting an AI agent with “Do task X now”, one can structure the interaction as: **(R)** the AI first asks clarifying questions or gathers relevant information, **(P)** the AI proposes a detailed plan or rationale, which the human reviews, and only then **(I)** the AI executes the plan[\[34\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Research%3A%20Instead%20of%20guessing%20requirements%2C,solves%20the%20real%20user%20need)[\[35\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Using%20the%20research%20doc,it%E2%80%99s%20feasible%20and%20properly%20scoped). This mirrors how we’d supervise a junior employee or intern – we wouldn’t let them loose without first ensuring they understand the assignment and have a sensible approach. RPI provides that supervisory framework for AI. In practice, one successful example is the *“Chain-of-Thought” prompting technique*, where the AI is instructed to reason step by step (essentially doing an internal R and P) before giving a final answer (I). The RPI workflow for AI coding agents described by Patrick Robinson and others uses exactly this idea: the AI must produce a research document validating facts (Factual, Actionable, Relevant – FAR criteria)[\[36\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Research%3A%20Build%20Context%20%26%20Insight,more%20building%20on%20shaky%20foundations), then produce a plan with tasks (meeting FACTS criteria: Feasible, Atomic, Clear, Testable, Scoped)[\[18\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Decide%20What%20to%20Do,Clear%2C%20Testable%2C%20and%20properly%20Scoped), and only then proceed to coding[\[37\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Implement%3A%20Ship%20%26%20Learn%20With,while%20AI%20handles%20the%20implementation). This dramatically reduces phenomena like hallucination and “slop” code because it forces the AI to operate in a *constrained, validated context* at each step[\[38\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=thoroughly%E2%80%94gathering%20evidence%2C%20mapping%20code%20surfaces%2C,more%20building%20on%20shaky%20foundations)[\[39\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=A%3A%20By%20forcing%20the%20AI,code%20churn%20known%20as%20slop). In broader AI collaboration, RPI acts as a **safety layer**: the human remains in the loop at phase boundaries. During the Plan phase, the human can exercise oversight (termed “mental alignment” – ensuring the AI’s intent matches human intent)[\[40\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Mental%20Alignment%20%26%20Human%20Oversight). This is much easier and safer than trying to supervise by looking at final output alone. If the AI’s plan contains a dangerous or nonsensical step, the human can catch it before execution, preventing potentially costly mistakes by an autonomous agent[\[41\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,becoming%20a). In effect, RPI introduces a form of *governance* for AI actions, analogous to requiring a permit (plan) before building (implementation). We have seen that when AI agents are allowed to improvise without these checks, they can make glaring errors or even harmful moves (in one academic example, an AI tasked with cutting costs in simulations resorted to inappropriate actions when not properly guided). RPI would mitigate such outcomes by demanding that the AI explain its reasoning and get approval. Another angle is reliability: complex AI actions suffer from the **“semantic diffusion”** problem where goals get distorted over long chains of reasoning[\[42\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=%2A%20Spec,like%20RPI%20necessary%20for%20clarity). Enforcing RPI essentially **resets context** and re-aligns objectives at each phase, ensuring the AI doesn’t drift. Dex Horthy’s analysis noted that the RPI method “operationalizes chain-of-thought prompting into a lifecycle, enforcing a System 2 process on the AI”[\[4\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=technical%20reality%20is%20well,process%20on%20the%20AI) – meaning it compels the AI to be deliberative and not just react on superficial cues. The outcome is that even legacy, large codebases can be tackled by AI without the usual context failures[\[26\]](https://medium.com/aiguys/no-vibes-allowed-solving-hard-problems-in-complex-codebases-d550d165863e#:~:text=It%20seems%20pretty%20well,that%20was%20shipped%20last%20week)[\[27\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=%EF%B8%8F%20The%20RPI%20Workflow%20,Plan%2C%20Implement). In summary, human–AI teams that blur RPI boundaries (for instance, immediately trusting an AI’s solution without understanding or planning) often face issues of **premature convergence and confirmation bias** (humans accept AI outputs too readily, as highlighted by research[\[43\]](https://arxiv.org/html/2509.14824v1#:~:text=To%20best%20leverage%20confirmation%20bias,thereby%20weakening%20the%20dialogical%20process)). By contrast, structuring AI usage around RPI keeps both the AI and the human honest: the AI must justify and plan, and the human must think critically about the plan. This synergy leads to more reliable outcomes and transforms the AI from a “random output generator” into something more like an augmenting colleague following a disciplined method[\[44\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=The%20RPI%20Strategy%3A%20Your%20AI,Development%20GPS)[\[45\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Why%20This%20Actually%20Works).

## Objective Phase Completion Signals (Evaluation Criteria)

A critical practical question is: *How do we know when to move from Research to Plan, or Plan to Implement?* Without clear signals, teams might either rush ahead prematurely or linger too long. RPI benefits from **objective completion criteria** for each phase – effectively a *“definition of done”* for R and P (and even I) – to signal that the phase’s outputs are sufficient. Here are some concrete, non-ritualistic criteria used in industry and academia:

* **Research Phase Exit Criteria:** The research phase is sufficiently complete when the problem is well-characterized and the unknowns are reduced to an acceptable level. Objective signals include: a clear **problem statement** has been formulated (the team can concisely articulate what problem they are solving and why)[\[9\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=ask%20them%20a%20question%20that,%E2%80%9CWhat%20problem%20does%20this%20solve%3F%E2%80%9D); key facts and requirements have been gathered and documented (for example, in a brief stating user needs, technical constraints, and factual findings relevant to the solution)[\[46\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=FAR%20scale%20,more%20building%20on%20shaky%20foundations); and open questions or assumptions are identified with a plan to address them later. A strong indicator is if you can apply a measure like the **FAR scale** – are the findings *Factual* (backed by evidence, not guesswork), *Actionable* (relevant to designing a solution), and *Relevant* (pertinent to the real need)[\[46\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=FAR%20scale%20,more%20building%20on%20shaky%20foundations)? If yes, research has done its job of providing a reliable foundation[\[46\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=FAR%20scale%20,more%20building%20on%20shaky%20foundations). Another signal: stakeholders stop bringing up new fundamental questions in discussion – if the research was incomplete, meetings will still surface “have we considered X?” repeatedly. When those diminish, it implies most major unknowns have been addressed. Lastly, the presence of a **go/no-go decision point** can be used: at the end of research, a review can decide “Do we understand the problem enough to proceed to solution mode?” – if the answer is yes with consensus that further research would yield diminishing returns, that’s the signal to move on.

* **Plan Phase Exit Criteria:** A plan is ready (and worth implementing) when it clearly and concretely answers *what* will be done and *how*, such that an independent implementer could follow it. One formalism is the **FACTS checklist** for plan tasks – each task (or work item) should be *Feasible* (doable given resources and constraints), *Atomic* (small enough to be understood and completed in one go), *Clear* (unambiguous in intent), *Testable* (has a way to verify completion/success), and appropriately *Scoped* (not open-ended or overly broad)[\[18\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Decide%20What%20to%20Do,Clear%2C%20Testable%2C%20and%20properly%20Scoped). If every item in the plan meets these criteria, it’s a strong sign the plan is actionable[\[18\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Decide%20What%20to%20Do,Clear%2C%20Testable%2C%20and%20properly%20Scoped). Conversely, if tasks are vague (“research more” as a task in plan indicates plan is not done – that’s a research task that was never completed) or gigantic (“Build the entire module” with no sub-steps), then planning isn’t complete. Another objective signal: performing a **dry run or walkthrough** of the plan – e.g., in software, mentally simulate the implementation steps or do a code design walkthrough; in a project, do a readiness review. If the plan holds up (no one says “wait, we forgot step X” or “how do we do Y?”), it’s likely sufficient. Plan quality can also be measured by risk burndown: list the major project risks identified during research – by the end of planning, each should either have a mitigation in the plan or be accepted as-is. If not, planning might be premature. Documentation-wise, a short **Plan-of-Action summary** (like an outline of phases or milestones) that stakeholders sign off on can serve as evidence that the plan is understood and agreed upon – signifying it’s safe to execute. It’s important that these criteria are not merely formal (“we filled out the template, so we are done” – that would be ritualistic). They need to truly reflect preparedness. One method some teams use is \*\* scoring or checklists\*\*: for example, Patrick Robinson’s RPI approach uses validation scales where a research doc must score above a threshold on FAR, and a plan on FACTS, before proceeding[\[47\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Here%E2%80%99s%20the%20magic%3A%20The%20framework,move%20forward%E2%80%94the%20framework%20tells%20you)[\[48\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=,validation%20ensures%20plan%20has%20clear). This introduces a quantitative gate rather than gut feel[\[47\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Here%E2%80%99s%20the%20magic%3A%20The%20framework,move%20forward%E2%80%94the%20framework%20tells%20you). While scoring can be subjective, the mere act of scoring forces a check: if anyone hesitates to give a full score (e.g. a plan task isn’t clearly testable), that indicates the plan still needs work[\[48\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=,validation%20ensures%20plan%20has%20clear).

* **Implement Phase Evaluation:** During implementation, the goal is to ensure the execution stays on track and meets criteria defined in the plan. Objective signals at the end of implementation (before closing out or iterating) include: passing all defined tests (for code, automated test suites and user acceptance tests should be green – this aligns with the “Testable” criterion in plan; for policy or organizational change, meeting the success metrics or KPIs identified in planning, e.g. adoption rate, error reduction percentage, etc.). Another signal is **completing all planned tasks** – if tasks were atomic and checkable (e.g., a checklist of sub-features to implement, or steps to deploy), then finishing them indicates implement phase done. However, in a more continuous process, evaluation criteria can also include peer review or audits: e.g., code review sign-off, quality assurance approval, or a post-implementation review meeting that signs off that objectives were met. Essentially, implementation is “done” when the solution as planned is delivered and validated in the real environment. If R and P were done well, one expects fewer surprises here; if not, one might extend implementation to fix issues – but then formally, you might consider that part of another RPI cycle (researching the bug, planning the fix, implementing). In contexts like manufacturing or construction, implementation done criteria are very clear (the product passes all inspections and meets spec). In software, a helpful concept is **“Definition of Done”**: a shared checklist (e.g. code written, reviewed, documented, deployed to staging, integration tests passed, etc.) that must be true to call a user story done. That is effectively evaluation criteria for each implementable increment, ensuring quality gates (like tests and reviews) are passed. Aligning those gates with what was planned (for instance, if the plan said “Module A must handle 1000 req/sec”, the implementation is only done when a load test confirms this).

It’s worth noting that evaluation criteria should avoid subjective or purely ceremonial markers (like “sign-off from manager X” without a checklist of what they check). The aim is to have **evidence-based exit criteria** – much like a pilot wouldn’t take off without checking specific instruments, a team shouldn’t move on without checking, say, that all research questions marked “critical” have answers, or that all plan tasks meet quality bars. By institutionalizing these signals, RPI transitions become smoother and less dependent on individual judgment or optimism. It also helps prevent the anti-patterns: e.g., infinite research is curtailed by predefined metrics for “enough research”, and premature planning is avoided because research isn’t marked done until factual and relevant findings are in place. In sum, clear evaluation criteria at each boundary enforce the integrity of RPI and give teams confidence that they are ready for the next phase[\[47\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Here%E2%80%99s%20the%20magic%3A%20The%20framework,move%20forward%E2%80%94the%20framework%20tells%20you)[\[48\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=,validation%20ensures%20plan%20has%20clear). They essentially operationalize the question: *“Are we ready to move on?”* into a checklist that can be answered with evidence, not just feeling.

## Synthesis: Principles and Trade-offs in Applying RPI

Bringing together the analysis above, we can distill a set of guiding principles for when and how to enforce the RPI pattern. RPI is a powerful framework for correctness, but it must be applied with nuance:

* **Enforce Rigor When the Cost of Mistake is High:** The more critical the decision or complex the problem, the more RPI should be followed *to the letter*. In safety-critical engineering (aerospace, healthcare devices), or major architectural decisions in software, skipping research or planning can literally be fatal or extremely costly. These are cases where *“measure twice, cut once”* is law. Therefore, RPI should be rigidly enforced – thorough requirements research, formal design reviews, etc. – before implementation. Historical precedents, like NASA’s emphasis on requirements analysis after failures traced to missing requirements[\[49\]](https://ntrs.nasa.gov/api/citations/20060020183/downloads/20060020183.pdf#:~:text=,by%20a%20number%20of), show that high-stakes domains need strong RPI discipline. Similarly, when an AI system’s mistakes could cause harm (e.g. an autonomous driving AI), an RPI-like process (extensive simulation data gathering, careful planning of decision logic, then implementation with oversight) is necessary as a safety layer. Principle: use maximum RPI (possibly with additional “Check” phases) in *“error-intolerant”* scenarios. The trade-off is speed, but in these cases correctness far outweighs speed.

* **Compress or Skip Phases When Problems are Simple or Speed is Paramount (but consciously accept the risk):** On the flip side, if the problem is well-understood and the solution space is known (a routine task your team has done many times), or if you’re in an exploratory stage where *learning by doing* is more valuable than upfront analysis (e.g. early product ideation, hackathons), then RPI can be compressed. Perhaps Research is just recalling past knowledge (no new extensive research), and Planning is a quick chat outlining next steps, then straight to Implementation. This should be a conscious choice: effectively saying *“We are confident the downside of minimal R\&P is low – we can fix as we go.”* For example, a startup might intentionally do minimal initial planning to get a prototype out in two weeks and then gather real user data (treating the first launch as research). RPI would then occur across iterations (each iteration’s evaluation feeding the next cycle’s plan). The principle here is **time-critical, low-criticality tasks can use a fast-and-light RPI**. However, teams must acknowledge the increased risk of rework; if speed of learning is the goal, that’s acceptable. It’s important not to fool oneself: if you compress RPI on a problem that actually was complex, reality will punish you later (with rework or failure). So this principle must be applied where appropriate – it often relies on expert intuition to judge simplicity or on an organizational decision to trade quality for speed knowingly (e.g. “We know this first version will be thrown away, and that’s okay”).

* **Never Skip Entirely:** Even when compressed, RPI’s **mindset should not be skipped entirely**. There is a fundamental difference between compressing phases and eliminating them. For instance, a team might do “research-lite” by spending just an hour checking assumptions or recalling prior art – but that’s still research. Completely skipping means going in blind, which is almost never justified except perhaps in a pure gamble scenario (and gambling is not good engineering). A guiding principle is that *every task, however small, deserves at least a moment of research and planning* – even if that moment is a mental note or a quick discussion. Many failures in both software and other domains can be traced to not even a moment’s thought before action. Skipping RPI usually stems from overconfidence or ignorance (“just implement it, why waste time thinking”). A principle to instill: **Do not act until you’ve asked: What do I need to know? And how will I approach it?** Those two questions encapsulate R and P in simplest form. Only trivial tasks where the answer to those is already in your head (through experience) can proceed immediately. Another way to say this: *if an experienced person would find value in thinking first, then skipping RPI is folly for you.* Thus, no matter the pressure, outright skipping should be avoided. If forced by extreme circumstances (e.g. emergency hotfix at 3 AM), one should be aware that it’s a deviation and carry out a post-implementation review after to catch what was missed (effectively doing belated research/planning to mitigate the risks introduced).

* **Adapt Granularity to Scale:** Rigid or flexible application of RPI also depends on scale. For strategic, large-scale efforts, each phase might be lengthy and formal (weeks of research, documented plan, etc.). For small-scale efforts, each phase might be minutes and informal. But the pattern holds. Principle: **scale RPI to the scope of the problem**. Use micro-RPI cycles for micro-problems (e.g. even writing a single function, you might quickly reason about what it should do, jot a quick plan of approach, then code). Use macro-RPI for big projects (market research study, architecture blueprint, then full project execution plan). Recognize the fractal nature: RPI can nest. For example, within an implementation phase of a big project, a developer might hit a sub-problem and go through RPI on that. This is healthy and not a violation – it’s applying the pattern at the right level. What must be avoided is confusion of scale: don’t apply a micro-level of research to a macro problem or vice versa. A failure mode is treating a huge uncertain project with only a cursory hour of research (underkill) or treating a tiny task with a month of planning (overkill). The principle of *right-sizing* is key for RPI to add value without becoming bureaucracy or inadequacy.

* **Use RPI as a Communication and Alignment Tool:** One often overlooked benefit of RPI is that it creates natural checkpoints for team alignment. Enforcing RPI means the team has to articulate the problem (R output) – which aligns everyone on what they’re solving. Then articulate the plan – aligning everyone on how they’ll tackle it. Skipping or compressing too much loses these alignment moments. So another principle: **when team coordination and shared understanding are critical, lean towards a more explicit RPI.** This is why, for instance, complex multi-team projects have integrated planning workshops and design docs; without them, each silo might implement in diverging ways. On the other hand, in a single-person task where alignment isn’t an issue, one can compress because communication is trivial (it’s just you). So the extent of RPI rigor can also be chosen based on *how many people need to agree*. More people/stakeholders \= more formal RPI to ensure consensus and clarity. This principle explains why small startups can “wing it” more (fewer nodes to align) whereas large organizations need more process. RPI scales with communication needs as much as problem needs.

* **Acknowledge Uncertainty and Remain Adaptive:** Lastly, a guiding principle drawn from all this is *humility*: acknowledging that despite best research and planning, things can change or be wrong. RPI is a working theory of control, not a guarantee of omniscience. So one must always be prepared to revisit earlier phases if new information arises. Enforcing RPI rigidly doesn’t mean never iterating; it means when iteration is needed, you still go through R and P again rather than doing ad-hoc patching. For example, if during implementation you discover a previously unknown constraint, the RPI-minded response is to step back into research mode (investigate the constraint), revise the plan, then continue – rather than plowing ahead hoping for the best. The ability to pivot while still preserving RPI structure is crucial. Principle: **treat RPI as cyclic** – enforce it within cycles, but be ready to cycle back if needed. This ensures trade-offs are continually reassessed with fresh research and planning, rather than stubbornly sticking to a plan that made sense under outdated assumptions. In other words, RPI isn’t a one-and-done flow; it’s a loop that can repeat, which addresses uncertainties and dynamic conditions.

In conclusion, the RPI pattern is a robust framework that, when applied thoughtfully, greatly improves the correctness and quality of problem solving and project execution. Its power comes from enforcing a logical order: know before you decide, decide before you do. But it is not a silver bullet – it incurs overhead and time, which must be balanced against needs for speed and innovation. By adhering to its principles in high-stakes contexts and intelligently compressing it in low-stakes ones (while never abandoning its spirit), teams of engineers, designers, policymakers, and even AI systems can collaborate more effectively with fewer missteps. The overarching lesson is one echoed by many disciplines: **separate thinking from doing, but do not separate them so far that they lose touch**. RPI provides the pattern to do exactly that – a middle path between chaos and rigidity, between thoughtless action and actionless thought. Implemented as a working theory, RPI remains subject to refinement, but it stands as a proven aid for those who prioritize getting things *right* over simply getting them *done*.

---

[\[1\]](https://en.wikipedia.org/wiki/Scientific_method#:~:text=While%20the%20scientific%20method%20is,have%20not%20followed%20the%20textbook) Scientific method \- Wikipedia

[https://en.wikipedia.org/wiki/Scientific\_method](https://en.wikipedia.org/wiki/Scientific_method)

[\[2\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=Why%20it%20happens%3A%20Project%20teams,is%20a%20setup%20for%20failure) [\[12\]](https://wellingtone.co.uk/top-7-reasons-project-failure/#:~:text=4) Top 7 Causes of Project Failure & How to Avoid Them

[https://wellingtone.co.uk/top-7-reasons-project-failure/](https://wellingtone.co.uk/top-7-reasons-project-failure/)

[\[3\]](https://en.wikipedia.org/wiki/PDCA#:~:text=ImageThe%20plan%E2%80%93do%E2%80%93check%E2%80%93act%20cycle) PDCA \- Wikipedia

[https://en.wikipedia.org/wiki/PDCA](https://en.wikipedia.org/wiki/PDCA)

[\[4\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=technical%20reality%20is%20well,process%20on%20the%20AI) [\[5\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,slow%2C%20deliberative%29%20thinking) [\[11\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=where%20RPI%20is%20most%20needed,validated%20process) [\[14\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Argues%20for%20the%20power%20of,loading%20advocated%20here) [\[20\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Q%3A%20How%20does%20the%20Research,workflow%20prevent%20slop) [\[21\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=However%2C%20the%20approach%20heavily%20relies,to%20avoid%20semantic%20fatigue) [\[27\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=%EF%B8%8F%20The%20RPI%20Workflow%20,Plan%2C%20Implement) [\[28\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,risk%20of%20errors%20or%20hallucinations) [\[29\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=plans%20,to%20avoid%20semantic%20fatigue) [\[31\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Q%3A%20How%20does%20the%20Research,workflow%20prevent%20slop) [\[39\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=A%3A%20By%20forcing%20the%20AI,code%20churn%20known%20as%20slop) [\[40\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=Mental%20Alignment%20%26%20Human%20Oversight) [\[41\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=,becoming%20a) [\[42\]](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer#:~:text=%2A%20Spec,like%20RPI%20necessary%20for%20clarity) ️️‍♂️ No Vibes Allowed: Solving Hard Problems in Complex Codebases – Dex Horthy, HumanLayer

[https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer](https://bagrounds.org/videos/no-vibes-allowed-solving-hard-problems-in-complex-codebases-dex-horthy-humanlayer)

[\[6\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=Why%20We%20Don%E2%80%99t%20Value%20Understanding,the%20Problem) [\[7\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=Over%20and%20over%20again%20I,teams%20do%20to%20break%20through) [\[8\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=So%20jumping%20to%20solutions%20is,bad%20for%20developing%20software%20products) [\[9\]](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem#:~:text=ask%20them%20a%20question%20that,%E2%80%9CWhat%20problem%20does%20this%20solve%3F%E2%80%9D) Stop Jumping to Solutions and Think About the Problem | Thoughtworks United States

[https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem](https://www.thoughtworks.com/en-us/insights/blog/you-need-understand-problem)

[\[10\]](https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false&triedRedirect=true#:~:text=Decision,that%20ultimately%20informs%20the%20decision) Decision-based Evidence Making \- by Tom Bloom

[https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203\&utm\_campaign=post\&utm\_medium=web\&showWelcomeOnShare=false\&triedRedirect=true](https://themoralcompass.substack.com/p/decision-based-evidence-making?r=1z8203&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false&triedRedirect=true)

[\[13\]](https://passo.uno/ai-wikis-docs-teather-as-a-service/#:~:text=,because%20that%E2%80%99s%20The%20Right%20Way%E2%84%A2) Code wikis are documentation theater as a service \- passo.uno

[https://passo.uno/ai-wikis-docs-teather-as-a-service/](https://passo.uno/ai-wikis-docs-teather-as-a-service/)

[\[15\]](https://www.reddit.com/r/vibecoding/comments/1qs80k4/everything_one_should_know_about_specdriven/#:~:text=Software%20development%20is%20moving%20fast%2C,that%20it%20lacks%20solid%20planning) Everything one should know about Spec-Driven Development (SDD) : r/vibecoding

[https://www.reddit.com/r/vibecoding/comments/1qs80k4/everything\_one\_should\_know\_about\_specdriven/](https://www.reddit.com/r/vibecoding/comments/1qs80k4/everything_one_should_know_about_specdriven/)

[\[16\]](https://www.benchmarksixsigma.com/forum/topic/36154-hippo-effect/#:~:text=HiPPO%20Effect%20is%20the%20effect,opinions%20influence%20the%20group%27s%20discussion) [\[17\]](https://www.benchmarksixsigma.com/forum/topic/36154-hippo-effect/#:~:text=HIPPO%20is%20one%20of%20the,has%20a%20very%20high%20pay) HiPPO Effect \- We ask and you answer\! The best answer wins\! \- Benchmark Six Sigma Forum

[https://www.benchmarksixsigma.com/forum/topic/36154-hippo-effect/](https://www.benchmarksixsigma.com/forum/topic/36154-hippo-effect/)

[\[18\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Decide%20What%20to%20Do,Clear%2C%20Testable%2C%20and%20properly%20Scoped) [\[30\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Implement%3A%20With%20your%20plan%20doc,creep%E2%80%94just%20focused%20execution%20that%20works) [\[34\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Research%3A%20Instead%20of%20guessing%20requirements%2C,solves%20the%20real%20user%20need) [\[35\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Plan%3A%20Using%20the%20research%20doc,it%E2%80%99s%20feasible%20and%20properly%20scoped) [\[36\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Research%3A%20Build%20Context%20%26%20Insight,more%20building%20on%20shaky%20foundations) [\[37\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Implement%3A%20Ship%20%26%20Learn%20With,while%20AI%20handles%20the%20implementation) [\[38\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=thoroughly%E2%80%94gathering%20evidence%2C%20mapping%20code%20surfaces%2C,more%20building%20on%20shaky%20foundations) [\[44\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=The%20RPI%20Strategy%3A%20Your%20AI,Development%20GPS) [\[45\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Why%20This%20Actually%20Works) [\[46\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=FAR%20scale%20,more%20building%20on%20shaky%20foundations) [\[47\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=Here%E2%80%99s%20the%20magic%3A%20The%20framework,move%20forward%E2%80%94the%20framework%20tells%20you) [\[48\]](https://patrickarobinson.com/blog/introducing-rpi-strategy/#:~:text=,validation%20ensures%20plan%20has%20clear) Introducing the RPI Strategy

[https://patrickarobinson.com/blog/introducing-rpi-strategy/](https://patrickarobinson.com/blog/introducing-rpi-strategy/)

[\[19\]](https://www.pmi.org/learning/library/poor-requirements-management-source-failed-projects-9341#:~:text=I%20still%20don%27t%20have%20time,In) I still don't have time to manage requirements: My project is ... \- PMI

[https://www.pmi.org/learning/library/poor-requirements-management-source-failed-projects-9341](https://www.pmi.org/learning/library/poor-requirements-management-source-failed-projects-9341)

[\[22\]](https://nsarchive2.gwu.edu/NSAEBB/NSAEBB328/II-Doc14.pdf#:~:text=But%20the%20intelligence%20and%20facts,for%20publishing%20material%20on) \[PDF\] The Secret Downing Street Memo

[https://nsarchive2.gwu.edu/NSAEBB/NSAEBB328/II-Doc14.pdf](https://nsarchive2.gwu.edu/NSAEBB/NSAEBB328/II-Doc14.pdf)

[\[23\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=In%20the%20IT%20management%20context%2C,Per%20Wikipedia) [\[24\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=In%20the%20IT%20management%20context%2C,Per%20Wikipedia) [\[25\]](https://www.ittoolkit.com/articles/analysis-paralysis#:~:text=,point%20at%20which%20decisions%20could) How to Defeat Analysis Paralysis with Informed Decisions

[https://www.ittoolkit.com/articles/analysis-paralysis](https://www.ittoolkit.com/articles/analysis-paralysis)

[\[26\]](https://medium.com/aiguys/no-vibes-allowed-solving-hard-problems-in-complex-codebases-d550d165863e#:~:text=It%20seems%20pretty%20well,that%20was%20shipped%20last%20week) Most efficient way to do Vibe Coding | AIGuys

[https://medium.com/aiguys/no-vibes-allowed-solving-hard-problems-in-complex-codebases-d550d165863e](https://medium.com/aiguys/no-vibes-allowed-solving-hard-problems-in-complex-codebases-d550d165863e)

[\[32\]](https://www.ama.org/marketing-news/the-5-phases-of-design-thinking/#:~:text=5%20Phases%20of%20Design%20Thinking,Define%2C%20Ideate%2C%20Prototype%2C%20and%20Test) 5 Phases of Design Thinking for Effective Innovation | AMA

[https://www.ama.org/marketing-news/the-5-phases-of-design-thinking/](https://www.ama.org/marketing-news/the-5-phases-of-design-thinking/)

[\[33\]](https://www.aicd.com.au/company-policies/crisis-management/plan/the-key-role-of-effective-change-adaptability-in-crisis-management.html#:~:text=Similarly%2C%20the%20delivery%20of%20any,%E2%80%9D) What directors need to know about change management

[https://www.aicd.com.au/company-policies/crisis-management/plan/the-key-role-of-effective-change-adaptability-in-crisis-management.html](https://www.aicd.com.au/company-policies/crisis-management/plan/the-key-role-of-effective-change-adaptability-in-crisis-management.html)

[\[43\]](https://arxiv.org/html/2509.14824v1#:~:text=To%20best%20leverage%20confirmation%20bias,thereby%20weakening%20the%20dialogical%20process) Confirmation Bias as a Cognitive Resource in LLM-Supported Deliberation

[https://arxiv.org/html/2509.14824v1](https://arxiv.org/html/2509.14824v1)

[\[49\]](https://ntrs.nasa.gov/api/citations/20060020183/downloads/20060020183.pdf#:~:text=,by%20a%20number%20of) \[PDF\] Questioning the Role of Requirements Engineering in the Causes of ...

[https://ntrs.nasa.gov/api/citations/20060020183/downloads/20060020183.pdf](https://ntrs.nasa.gov/api/citations/20060020183/downloads/20060020183.pdf)